{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ManojKumarKolli/ManojKumar_INFO5731_Spring2024/blob/main/In_class_exercise/Kolli_ManojKumar_Exercise_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DymRJbxDBCnf"
      },
      "source": [
        "# **INFO5731 In-class Exercise 2**\n",
        "\n",
        "The purpose of this exercise is to understand users' information needs, and then collect data from different sources for analysis by implementing web scraping using Python.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBKvD6O_TY6e"
      },
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting research question (or practical question or something innovative) you have in mind, what kind of data should be collected to answer the question(s)? Specify the amount of data needed for analysis. Provide detailed steps for collecting and saving the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cikVKDXdTbzE"
      },
      "outputs": [],
      "source": [
        "# write your answer here\n",
        "\"\"\"\n",
        "Research Question: Explore how public sentiment towards renewable energy technologies (solar, wind, hydroelectric) varies by region and changes over time on social media.\n",
        "\n",
        "Data Required:\n",
        "\n",
        "Social Media Posts: Related to solar, wind, and hydroelectric energy.\n",
        "Sentiment Data: Sentiment scores (positive, neutral, negative) for each post.\n",
        "Geographic and Temporal Data: Location (if available) and timestamps of posts.\n",
        "Amount of Data: Minimum 3,000 samples (1,000 for each energy type) for a comprehensive analysis.\n",
        "\n",
        "Collection Steps:\n",
        "\n",
        "API Access: Obtain access to a social media platform's API, such as Reddit.\n",
        "Data Collection: Use the API to collect posts mentioning specific renewable energy keywords, capturing text, sentiment, location, and time.\n",
        "Data Storage: Save the data in a structured format, like a CSV file, with columns for each required piece of information.\n",
        "Ethical Considerations: Follow the platform's data use policy and consider user privacy.\n",
        "\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9RqrlwdTfvl"
      },
      "source": [
        "## Question 2 (10 Points)\n",
        "Write Python code to collect a dataset of 1000 samples related to the question discussed in Question 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybCEuV8aJZ_G",
        "outputId": "7e1b1371-63bf-4c95-90fb-8af78c98cefc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting praw\n",
            "  Downloading praw-7.7.1-py3-none-any.whl (191 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/191.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/191.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m191.0/191.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting prawcore<3,>=2.1 (from praw)\n",
            "  Downloading prawcore-2.4.0-py3-none-any.whl (17 kB)\n",
            "Collecting update-checker>=0.18 (from praw)\n",
            "  Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.10/dist-packages (from praw) (1.7.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from prawcore<3,>=2.1->praw) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2024.2.2)\n",
            "Installing collected packages: update-checker, prawcore, praw\n",
            "Successfully installed praw-7.7.1 prawcore-2.4.0 update-checker-0.18.0\n"
          ]
        }
      ],
      "source": [
        "pip install praw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "4XvRknixTh1g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cad2c84-c0cd-4eb3-da64-fe3d387f9c4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                title  \\\n",
            "0   Why solar energy will continue to lead the pac...   \n",
            "1         The momentum of the solar energy transition   \n",
            "2   Solar Energy Sites Nurture Blooming Insect Pop...   \n",
            "3   The Annular Solar Eclipse Will Decimate US Sol...   \n",
            "4     Harvesting more solar energy with supercrystals   \n",
            "5       Minnesota approves giant solar energy project   \n",
            "6   Dillon solar energy farm ushers in clean energ...   \n",
            "7   Solar energy in the U.S. may triple in five years   \n",
            "8   Producing solar energy, strawberries, tomatoes...   \n",
            "9   How Solar Energy Can Help Boost Biodiversity A...   \n",
            "10  Local city government thwarting solar energy g...   \n",
            "11  Solar energy production tripled in Latvia over...   \n",
            "12  Solar energy companies say scammers are going ...   \n",
            "13  Bureau of Land Management analysis aims to opt...   \n",
            "14  Egypt constructs solar energy stations in muse...   \n",
            "15  500-mile-long solar energy power line find fos...   \n",
            "16  US could finance solar power in Mexico and imp...   \n",
            "17  Agrivoltaic: Harnessing the Synergy Between Ag...   \n",
            "18  Solar panels produce 200 times more energy per...   \n",
            "19  $230 million solar energy project coming to Ma...   \n",
            "\n",
            "                                                 text   created_utc  upvotes  \\\n",
            "0                                                      1.701786e+09      198   \n",
            "1                                                      1.697560e+09       56   \n",
            "2                                                      1.705919e+09       63   \n",
            "3                                                      1.697229e+09        0   \n",
            "4                                                      1.701587e+09       19   \n",
            "5                                                      1.695392e+09       62   \n",
            "6                                                      1.696445e+09       21   \n",
            "7                                                      1.662743e+09      278   \n",
            "8                                                      1.695759e+09       65   \n",
            "9                                                      1.693400e+09       84   \n",
            "10  I live in Arkansas. I live outside the limits ...  1.625624e+09      145   \n",
            "11                                                     1.695649e+09       55   \n",
            "12                                                     1.706421e+09       40   \n",
            "13                                                     1.705634e+09       40   \n",
            "14                                                     1.691938e+09       47   \n",
            "15                                                     1.678213e+09      132   \n",
            "16                                                     1.671800e+09      194   \n",
            "17                                                     1.691387e+09       18   \n",
            "18  “One analysis from Germany found potatoes coul...  1.652121e+09      429   \n",
            "19                                                     1.685818e+09       29   \n",
            "\n",
            "    num_comments  \n",
            "0             28  \n",
            "1             16  \n",
            "2              1  \n",
            "3              7  \n",
            "4              0  \n",
            "5              1  \n",
            "6              0  \n",
            "7             23  \n",
            "8              3  \n",
            "9              3  \n",
            "10            42  \n",
            "11             1  \n",
            "12             0  \n",
            "13             0  \n",
            "14             1  \n",
            "15            64  \n",
            "16           115  \n",
            "17             1  \n",
            "18            95  \n",
            "19             4  \n"
          ]
        }
      ],
      "source": [
        "# write your answer here\n",
        "import praw\n",
        "import pandas as pd\n",
        "\n",
        "reddit = praw.Reddit(client_id='Rm51RgkkORs6zhyg23rNcg',\n",
        "                     client_secret='G4yNrSb24cbkLUXm5frLVOxjji18Bg',\n",
        "                     user_agent='test')\n",
        "\n",
        "def collect_samples(subreddit, keyword, limit=1000):\n",
        "\n",
        "    posts = []\n",
        "    for submission in reddit.subreddit(subreddit).search(keyword, limit=limit):\n",
        "        posts.append({\n",
        "            'title': submission.title,\n",
        "            'text': submission.selftext,\n",
        "            'created_utc': submission.created_utc,\n",
        "            'upvotes': submission.score,\n",
        "            'num_comments': submission.num_comments\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(posts)\n",
        "    return df\n",
        "\n",
        "df_solar = collect_samples('renewableenergy', 'solar energy', 20)\n",
        "print(df_solar)\n",
        "df_solar.to_csv('solar_energy_posts.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03jb4GZsBkBS"
      },
      "source": [
        "## Question 3 (10 Points)\n",
        "Write Python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"XYZ\". The articles should be published in the last 10 years (2014-2024).\n",
        "\n",
        "The following information from the article needs to be collected:\n",
        "\n",
        "(1) Title of the article\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "YaGLbSHHB8Ej",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0269b9ef-e967-4b63-8c04-4bb6ca23521e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieved Articles:\n",
            "{'title': 'Spark NLP: Natural Language Understanding at Scale', 'venue/journal/conference': 'arxiv', 'year': '2021', 'authors': 'Veysel Kocaman, David Talby', 'abstract': 'Spark NLP is a Natural Language Processing (NLP) library built on top of\\nApache Spark ML. It provides simple, performant and accurate NLP annotations\\nfor machine learning pipelines that can scale easily in a distributed\\nenvironment. Spark NLP comes with 1100 pre trained pipelines and models in more\\nthan 192 languages. It supports nearly all the NLP tasks and modules that can\\nbe used seamlessly in a cluster. Downloaded more than 2.7 million times and\\nexperiencing nine times growth since January 2020, Spark NLP is used by 54% of\\nhealthcare organizations as the worlds most widely used NLP library in the\\nenterprise.'}\n",
            "{'title': 'Sejarah dan Perkembangan Teknik Natural Language Processing (NLP) Bahasa\\n  Indonesia: Tinjauan tentang sejarah, perkembangan teknologi, dan aplikasi NLP\\n  dalam bahasa Indonesia', 'venue/journal/conference': 'arxiv', 'year': '2023', 'authors': 'Mukhlis Amien', 'abstract': 'This study provides an overview of the history of the development of Natural\\nLanguage Processing (NLP) in the context of the Indonesian language, with a\\nfocus on the basic technologies, methods, and practical applications that have\\nbeen developed. This review covers developments in basic NLP technologies such\\nas stemming, part-of-speech tagging, and related methods; practical\\napplications in cross-language information retrieval systems, information\\nextraction, and sentiment analysis; and methods and techniques used in\\nIndonesian language NLP research, such as machine learning, statistics-based\\nmachine translation, and conflict-based approaches. This study also explores\\nthe application of NLP in Indonesian language industry and research and\\nidentifies challenges and opportunities in Indonesian language NLP research and\\ndevelopment. Recommendations for future Indonesian language NLP research and\\ndevelopment include developing more efficient methods and technologies,\\nexpanding NLP applications, increasing sustainability, further research into\\nthe potential of NLP, and promoting interdisciplinary collaboration. It is\\nhoped that this review will help researchers, practitioners, and the government\\nto understand the development of Indonesian language NLP and identify\\nopportunities for further research and development.'}\n",
            "{'title': 'The Law and NLP: Bridging Disciplinary Disconnects', 'venue/journal/conference': 'arxiv', 'year': '2023', 'authors': \"Robert Mahari, Dominik Stammbach, Elliott Ash, Alex 'Sandy' Pentland\", 'abstract': 'Legal practice is intrinsically rooted in the fabric of language, yet legal\\npractitioners and scholars have been slow to adopt tools from natural language\\nprocessing (NLP). At the same time, the legal system is experiencing an access\\nto justice crisis, which could be partially alleviated with NLP. In this\\nposition paper, we argue that the slow uptake of NLP in legal practice is\\nexacerbated by a disconnect between the needs of the legal community and the\\nfocus of NLP researchers. In a review of recent trends in the legal NLP\\nliterature, we find limited overlap between the legal NLP community and legal\\nacademia. Our interpretation is that some of the most popular legal NLP tasks\\nfail to address the needs of legal practitioners. We discuss examples of legal\\nNLP tasks that promise to bridge disciplinary disconnects and highlight\\ninteresting areas for legal NLP research that remain underexplored.'}\n",
            "{'title': 'A Survey on Recognizing Textual Entailment as an NLP Evaluation', 'venue/journal/conference': 'arxiv', 'year': '2020', 'authors': 'Adam Poliak', 'abstract': 'Recognizing Textual Entailment (RTE) was proposed as a unified evaluation\\nframework to compare semantic understanding of different NLP systems. In this\\nsurvey paper, we provide an overview of different approaches for evaluating and\\nunderstanding the reasoning capabilities of NLP systems. We then focus our\\ndiscussion on RTE by highlighting prominent RTE datasets as well as advances in\\nRTE dataset that focus on specific linguistic phenomena that can be used to\\nevaluate NLP systems on a fine-grained level. We conclude by arguing that when\\nevaluating NLP systems, the community should utilize newly introduced RTE\\ndatasets that focus on specific linguistic phenomena.'}\n",
            "{'title': 'A Survey of Race, Racism, and Anti-Racism in NLP', 'venue/journal/conference': 'arxiv', 'year': '2021', 'authors': 'Anjalie Field, Su Lin Blodgett, Zeerak Waseem, Yulia Tsvetkov', 'abstract': 'Despite inextricable ties between race and language, little work has\\nconsidered race in NLP research and development. In this work, we survey 79\\npapers from the ACL anthology that mention race. These papers reveal various\\ntypes of race-related bias in all stages of NLP model development, highlighting\\nthe need for proactive consideration of how NLP systems can uphold racial\\nhierarchies. However, persistent gaps in research on race and NLP remain: race\\nhas been siloed as a niche topic and remains ignored in many NLP tasks; most\\nwork operationalizes race as a fixed single-dimensional variable with a\\nground-truth label, which risks reinforcing differences produced by historical\\nracism; and the voices of historically marginalized people are nearly absent in\\nNLP literature. By identifying where and how NLP literature has and has not\\nconsidered race, especially in comparison to related fields, our work calls for\\ninclusion and racial justice in NLP research practices.'}\n",
            "{'title': 'NLP Methods in Host-based Intrusion Detection Systems: A Systematic\\n  Review and Future Directions', 'venue/journal/conference': 'arxiv', 'year': '2022', 'authors': 'Zarrin Tasnim Sworna, Zahra Mousavi, Muhammad Ali Babar', 'abstract': 'Host based Intrusion Detection System (HIDS) is an effective last line of\\ndefense for defending against cyber security attacks after perimeter defenses\\n(e.g., Network based Intrusion Detection System and Firewall) have failed or\\nbeen bypassed. HIDS is widely adopted in the industry as HIDS is ranked among\\nthe top two most used security tools by Security Operation Centers (SOC) of\\norganizations. Although effective and efficient HIDS is highly desirable for\\nindustrial organizations, the evolution of increasingly complex attack patterns\\ncauses several challenges resulting in performance degradation of HIDS (e.g.,\\nhigh false alert rate creating alert fatigue for SOC staff). Since Natural\\nLanguage Processing (NLP) methods are better suited for identifying complex\\nattack patterns, an increasing number of HIDS are leveraging the advances in\\nNLP that have shown effective and efficient performance in precisely detecting\\nlow footprint, zero day attacks and predicting the next steps of attackers.\\nThis active research trend of using NLP in HIDS demands a synthesized and\\ncomprehensive body of knowledge of NLP based HIDS. Thus, we conducted a\\nsystematic review of the literature on the end to end pipeline of the use of\\nNLP in HIDS development. For the end to end NLP based HIDS development\\npipeline, we identify, taxonomically categorize and systematically compare the\\nstate of the art of NLP methods usage in HIDS, attacks detected by these NLP\\nmethods, datasets and evaluation metrics which are used to evaluate the NLP\\nbased HIDS. We highlight the relevant prevalent practices, considerations,\\nadvantages and limitations to support the HIDS developers. We also outline the\\nfuture research directions for the NLP based HIDS development.'}\n",
            "{'title': 'We are Who We Cite: Bridges of Influence Between Natural Language\\n  Processing and Other Academic Fields', 'venue/journal/conference': 'arxiv', 'year': '2023', 'authors': 'Jan Philip Wahle, Terry Ruas, Mohamed Abdalla, Bela Gipp, Saif M. Mohammad', 'abstract': \"Natural Language Processing (NLP) is poised to substantially influence the\\nworld. However, significant progress comes hand-in-hand with substantial risks.\\nAddressing them requires broad engagement with various fields of study. Yet,\\nlittle empirical work examines the state of such engagement (past or current).\\nIn this paper, we quantify the degree of influence between 23 fields of study\\nand NLP (on each other). We analyzed ~77k NLP papers, ~3.1m citations from NLP\\npapers to other papers, and ~1.8m citations from other papers to NLP papers. We\\nshow that, unlike most fields, the cross-field engagement of NLP, measured by\\nour proposed Citation Field Diversity Index (CFDI), has declined from 0.58 in\\n1980 to 0.31 in 2022 (an all-time low). In addition, we find that NLP has grown\\nmore insular -- citing increasingly more NLP papers and having fewer papers\\nthat act as bridges between fields. NLP citations are dominated by computer\\nscience; Less than 8% of NLP citations are to linguistics, and less than 3% are\\nto math and psychology. These findings underscore NLP's urgent need to reflect\\non its engagement with various fields.\"}\n",
            "{'title': 'Natural Language Processing 4 All (NLP4All): A New Online Platform for\\n  Teaching and Learning NLP Concepts', 'venue/journal/conference': 'arxiv', 'year': '2021', 'authors': 'Rebekah Baglini, Arthur Hjorth', 'abstract': 'Natural Language Processing offers new insights into language data across\\nalmost all disciplines and domains, and allows us to corroborate and/or\\nchallenge existing knowledge. The primary hurdles to widening participation in\\nand use of these new research tools are, first, a lack of coding skills in\\nstudents across K-16, and in the population at large, and second, a lack of\\nknowledge of how NLP-methods can be used to answer questions of disciplinary\\ninterest outside of linguistics and/or computer science. To broaden\\nparticipation in NLP and improve NLP-literacy, we introduced a new tool\\nweb-based tool called Natural Language Processing 4 All (NLP4All). The intended\\npurpose of NLP4All is to help teachers facilitate learning with and about NLP,\\nby providing easy-to-use interfaces to NLP-methods, data, and analyses, making\\nit possible for non- and novice-programmers to learn NLP concepts\\ninteractively.'}\n",
            "{'title': 'Notes on Deep Learning for NLP', 'venue/journal/conference': 'arxiv', 'year': '2018', 'authors': 'Antoine J. -P. Tixier', 'abstract': 'My notes on Deep Learning for NLP.'}\n",
            "{'title': 'Translational NLP: A New Paradigm and General Principles for Natural\\n  Language Processing Research', 'venue/journal/conference': 'arxiv', 'year': '2021', 'authors': 'Denis Newman-Griffis, Jill Fain Lehman, Carolyn Rosé, Harry Hochheiser', 'abstract': 'Natural language processing (NLP) research combines the study of universal\\nprinciples, through basic science, with applied science targeting specific use\\ncases and settings. However, the process of exchange between basic NLP and\\napplications is often assumed to emerge naturally, resulting in many\\ninnovations going unapplied and many important questions left unstudied. We\\ndescribe a new paradigm of Translational NLP, which aims to structure and\\nfacilitate the processes by which basic and applied NLP research inform one\\nanother. Translational NLP thus presents a third research paradigm, focused on\\nunderstanding the challenges posed by application needs and how these\\nchallenges can drive innovation in basic science and technology design. We show\\nthat many significant advances in NLP research have emerged from the\\nintersection of basic principles with application needs, and present a\\nconceptual framework outlining the stakeholders and key questions in\\ntranslational research. Our framework provides a roadmap for developing\\nTranslational NLP as a dedicated research area, and identifies general\\ntranslational principles to facilitate exchange between basic and applied\\nresearch.'}\n",
            "\n",
            "****************************************************************************************************\n",
            "Collected 10 articles\n"
          ]
        }
      ],
      "source": [
        "# write your answer here\n",
        "import requests\n",
        "import xml.etree.ElementTree as ET\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "def fetch_arxiv_articles(keyword, max_results=1000):\n",
        "\n",
        "    articles = []\n",
        "    base_url = \"http://export.arxiv.org/api/query?\"\n",
        "    start = 0\n",
        "    max_per_query = 100\n",
        "\n",
        "    while len(articles) < max_results:\n",
        "        query_params = f\"search_query=all:{keyword}&start={start}&max_results={max_per_query}\"\n",
        "        response = requests.get(base_url + query_params)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            root = ET.fromstring(response.content)\n",
        "            for entry in root.findall('{http://www.w3.org/2005/Atom}entry'):\n",
        "                title = entry.find('{http://www.w3.org/2005/Atom}title').text.strip()\n",
        "                published = entry.find('{http://www.w3.org/2005/Atom}published').text[:4]  # Extract year\n",
        "                authors = [author.find('{http://www.w3.org/2005/Atom}name').text for author in\n",
        "                           entry.findall('{http://www.w3.org/2005/Atom}author')]\n",
        "                abstract = entry.find('{http://www.w3.org/2005/Atom}summary').text.strip()\n",
        "\n",
        "                articles.append({\n",
        "                    \"title\": title,\n",
        "                    \"venue/journal/conference\": \"arxiv\",\n",
        "                    \"year\": published,\n",
        "                    \"authors\": \", \".join(authors),\n",
        "                    \"abstract\": abstract\n",
        "                })\n",
        "\n",
        "                if len(articles) >= max_results:\n",
        "                    break\n",
        "            start += max_per_query\n",
        "        else:\n",
        "            print(f\"Failed to fetch data: {response.status_code}\")\n",
        "            break\n",
        "\n",
        "        time.sleep(3)\n",
        "\n",
        "    return articles\n",
        "\n",
        "\n",
        "keyword = \"NLP\"\n",
        "articles = fetch_arxiv_articles(keyword, 10)\n",
        "print(\"Retrieved Articles:\")\n",
        "for article in articles:\n",
        "    print(article)\n",
        "print()\n",
        "print(\"*\"*100)\n",
        "print(f\"Collected {len(articles)} articles\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have retrieved only 10 records instead of 10000 as we have limit constraints."
      ],
      "metadata": {
        "id": "ejk5DvlsMGkk"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJDe71iLB616"
      },
      "source": [
        "## Question 4A (10 Points)\n",
        "Develop Python code to collect data from social media platforms like Reddit, Instagram, Twitter (formerly known as X), Facebook, or any other. Use hashtags, keywords, usernames, or user IDs to gather the data.\n",
        "\n",
        "\n",
        "\n",
        "Ensure that the collected data has more than four columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MtKskTzbCLaU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3009540d-a076-4c4d-8453-9156200d6ae4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieved posts:\n",
            "{'title': 'Why the \"NLP\" that you were taught is not real NLP if you did NOT train with a Licensed NLP trainer.', 'url': 'https://v.redd.it/g1mppkz1kmqb1', 'created_utc': '2023-09-26 16:14:43', 'upvotes': 1, 'num_comments': 20, 'author': 'JoostvanderLeij'}\n",
            "{'title': 'NLP Power Words', 'url': 'https://www.reddit.com/r/NLP/comments/1ahk6lx/nlp_power_words/', 'created_utc': '2024-02-03 01:30:25', 'upvotes': 5, 'num_comments': 15, 'author': 'ihatewands'}\n",
            "{'title': 'NLP For Porn Addiction & Success In Life', 'url': 'https://www.reddit.com/r/NLP/comments/1722kyz/nlp_for_porn_addiction_success_in_life/', 'created_utc': '2023-10-07 09:53:46', 'upvotes': 5, 'num_comments': 69, 'author': 'Illustrious_Car5155'}\n",
            "{'title': 'NLP strategies for ADHD', 'url': 'https://www.reddit.com/r/NLP/comments/18mhsie/nlp_strategies_for_adhd/', 'created_utc': '2023-12-20 01:02:40', 'upvotes': 4, 'num_comments': 20, 'author': 'Capable-Breakfast480'}\n",
            "{'title': 'NLP for comedy', 'url': 'https://www.reddit.com/r/NLP/comments/1acd1ux/nlp_for_comedy/', 'created_utc': '2024-01-27 15:18:39', 'upvotes': 9, 'num_comments': 9, 'author': 'Environmental_Shoe80'}\n",
            "{'title': 'NLP for claustrophobia', 'url': 'https://www.reddit.com/r/NLP/comments/184wq1n/nlp_for_claustrophobia/', 'created_utc': '2023-11-27 06:44:23', 'upvotes': 1, 'num_comments': 15, 'author': 'Open-Sector2341'}\n",
            "{'title': 'Resources for NLP', 'url': 'https://www.reddit.com/r/NLP/comments/19479m8/resources_for_nlp/', 'created_utc': '2024-01-11 17:32:03', 'upvotes': 4, 'num_comments': 10, 'author': 'PrincipleOk6557'}\n",
            "{'title': 'NLP for dating ?', 'url': 'https://www.reddit.com/r/NLP/comments/174vm5j/nlp_for_dating/', 'created_utc': '2023-10-10 20:42:31', 'upvotes': 1, 'num_comments': 23, 'author': 'Vivid-Ad7048'}\n",
            "{'title': 'Will NLP work on me?', 'url': 'https://www.reddit.com/r/NLP/comments/187avb6/will_nlp_work_on_me/', 'created_utc': '2023-11-30 05:29:03', 'upvotes': 10, 'num_comments': 12, 'author': 'Parade2thegrave'}\n",
            "{'title': 'How to perform anchoring in NLP (psychology)', 'url': 'https://www.reddit.com/r/NLP/comments/1ae5ojw/how_to_perform_anchoring_in_nlp_psychology/', 'created_utc': '2024-01-29 20:44:19', 'upvotes': 8, 'num_comments': 10, 'author': 'George_Jackson85'}\n"
          ]
        }
      ],
      "source": [
        "import praw\n",
        "import datetime\n",
        "\n",
        "reddit = praw.Reddit(client_id='Rm51RgkkORs6zhyg23rNcg',\n",
        "                     client_secret='G4yNrSb24cbkLUXm5frLVOxjji18Bg',\n",
        "                     user_agent='test')\n",
        "\n",
        "\n",
        "def fetch_reddit_posts(subreddit_name, keyword, limit=100):\n",
        "\n",
        "    posts = []\n",
        "    subreddit = reddit.subreddit(subreddit_name)\n",
        "\n",
        "    # Search for submissions in the subreddit\n",
        "    for submission in subreddit.search(keyword, limit=limit):\n",
        "        post_details = {\n",
        "            'title': submission.title,\n",
        "            'url': submission.url,\n",
        "            'created_utc': datetime.datetime.utcfromtimestamp(submission.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
        "            'upvotes': submission.score,\n",
        "            'num_comments': submission.num_comments,\n",
        "            'author': str(submission.author)\n",
        "        }\n",
        "        posts.append(post_details)\n",
        "\n",
        "    return posts\n",
        "\n",
        "\n",
        "subreddit_name = 'NLP'\n",
        "keyword = 'nlp'\n",
        "posts = fetch_reddit_posts(subreddit_name, keyword, limit=10)\n",
        "\n",
        "print(\"Retrieved posts:\")\n",
        "for post in posts:\n",
        "    print(post)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55W9AMdXCSpV"
      },
      "source": [
        "## Question 4B (10 Points)\n",
        "If you encounter challenges with Question-4 web scraping using Python, employ any online tools such as ParseHub or Octoparse for data extraction. Introduce the selected tool, outline the steps for web scraping, and showcase the final output in formats like CSV or Excel.\n",
        "\n",
        "\n",
        "\n",
        "Upload a document (Word or PDF File) in any shared storage (preferably UNT OneDrive) and add the publicly accessible link in the below code cell.\n",
        "\n",
        "Please only choose one option for question 4. If you do both options, we will grade only the first one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I57NXsauCec2"
      },
      "outputs": [],
      "source": [
        "# write your answer here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZOhks1dXWEe"
      },
      "source": [
        "# Mandatory Question"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqmHVEwaWhbV"
      },
      "source": [
        "**Important: Reflective Feedback on Web Scraping and Data Collection**\n",
        "\n",
        "\n",
        "\n",
        "Please share your thoughts and feedback on the web scraping and data collection exercises you have completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on web scraping tasks. What were the key concepts or techniques you found most beneficial in understanding the process of extracting data from various online sources?\n",
        "\n",
        "\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in collecting data from certain websites, and how did you overcome them? If you opted for the non-coding option, share your experience with the chosen tool.\n",
        "\n",
        "\n",
        "\n",
        "Relevance to Your Field of Study: How might the ability to gather and analyze data from online sources enhance your work or research?\n",
        "\n",
        "**(no grading of your submission if this question is left unanswered)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akAVJn9YBTQT"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Learning experience: The overall experience with the web scraping is excellent. We learnt about the html tags and scrapping based on the component. With the knowledge gained we can collect the dataset from various sources.\n",
        "\n",
        "Challenges Encountered: We have some issue with the dynamic pages. So, we need to work more on the pagination in the query params.\n",
        "\n",
        "Relevance to Your Field of Study: As a data science student, we can collect the data on our own and employ our ML, DL and NLP models.\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}